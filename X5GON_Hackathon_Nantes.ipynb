{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOGM5kzalGpb"
   },
   "source": [
    "# Nantes Hackathon X5-GON preparatory questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PV7l5XJjywtj"
   },
   "source": [
    "Link to the hackathon website with all the useful resources and more: \n",
    "https://www.x5gon.org/event/ai-hackathon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVIgEPjP0J3l"
   },
   "source": [
    "## Cold start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee836pfgq644"
   },
   "source": [
    "### Download the catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "g140Ff9elyZm",
    "outputId": "4fa743c8-1a16-4fac-aeac-df1aeed29bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'x5gon_catelogue.tsv*': No such file or directory\n",
      "mkdir: cannot create directory ‘datasets’: File exists\n",
      "--2020-02-24 14:49:29--  https://gitlab.univ-nantes.fr/x5gon/x5gon-hackathon-datasets/raw/master/datasets/x5gon_catelogue.tsv\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving gitlab.univ-nantes.fr (gitlab.univ-nantes.fr)... 193.52.101.66\n",
      "Connecting to gitlab.univ-nantes.fr (gitlab.univ-nantes.fr)|193.52.101.66|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 63850145 (61M) [text/plain]\n",
      "Saving to: ‘x5gon_catelogue.tsv’\n",
      "\n",
      "x5gon_catelogue.tsv 100%[===================>]  60.89M  2.50MB/s    in 67s     \n",
      "\n",
      "2020-02-24 14:50:37 (929 KB/s) - ‘x5gon_catelogue.tsv’ saved [63850145/63850145]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! rm x5gon_catelogue.tsv*\n",
    "! mkdir datasets\n",
    "! wget https://gitlab.univ-nantes.fr/x5gon/x5gon-hackathon-datasets/raw/master/datasets/x5gon_catelogue.tsv\n",
    "! mv x5gon_catelogue.tsv datasets/catalogue.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "eYAlE7u_ryE3",
    "outputId": "18b613a1-55a6-4410-9560-85810281df0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\ttitle\ttype\tlanguage\tkeywords\tconcepts\n",
      "59260\tC7 - Computing with Space\ten\tpdf\t[space, omicini, c7, omicini disi, disi univ, disi, univ bologna, andrea omicini, computing, andrea, univ, bologna, agents, spatial, agent, coordination, tuple, space space, mas, computer science]\t['http://en.wikipedia.org/wiki/Bologna', 'http://en.wikipedia.org/wiki/Tuple', 'http://en.wikipedia.org/wiki/Computer_science', 'http://en.wikipedia.org/wiki/Democratic_Alignment_(2015)', 'http://en.wikipedia.org/wiki/Middleware', 'http://en.wikipedia.org/wiki/Distributed_computing', 'http://en.wikipedia.org/wiki/Geometry', 'http://en.wikipedia.org/wiki/Logic', 'http://en.wikipedia.org/wiki/Computer', 'http://en.wikipedia.org/wiki/Mathematics']\n",
      "3904\tElectromagnetic Fields, Forces, and Motion\ten\tpdf\t[forces motion, fields forces, electromagnetic fields, zahn page, markus zahn, prof markus, zahn, bs, forces, markus, fields, lecture prof, electromagnetic, dl, dt, sin, prof, tan, motion, dt dt]\t['http://en.wikipedia.org/wiki/Massachusetts_Institute_of_Technology', 'http://en.wikipedia.org/wiki/MIT_OpenCourseWare', 'http://en.wikipedia.org/wiki/Electromagnetism', 'http://en.wikipedia.org/wiki/Force', 'http://en.wikipedia.org/wiki/Motion', 'http://en.wikipedia.org/wiki/Please_(U2_song)', 'http://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki', 'http://en.wikipedia.org/wiki/Citation', 'http://en.wikipedia.org/wiki/Typesetting', 'http://en.wikipedia.org/wiki/Mark_the_Evangelist']\n",
      "4796\tUncertain Allies\ten\tpdf\t[north korea, korea, china, north, pyongyang, korean, kim, beijing, il, kim jong, north korean, jong, party, mao, sino, seoul, peninsula, ally, affairs, nuclear]\t['http://en.wikipedia.org/wiki/North_Korea', 'http://en.wikipedia.org/wiki/China', 'http://en.wikipedia.org/wiki/Pyongyang', 'http://en.wikipedia.org/wiki/Beijing', 'http://en.wikipedia.org/wiki/Korea', 'http://en.wikipedia.org/wiki/Yonsei_University', 'http://en.wikipedia.org/wiki/Seoul', 'http://en.wikipedia.org/wiki/South_Korea', 'http://en.wikipedia.org/wiki/Kim_Il-sung', 'http://en.wikipedia.org/wiki/United_States']\n",
      "5757\tStatistics for Brain and Cognitive Science\ten\tpdf\t[pr, pr pr, probability, probability theory, event, ms, outcome, page lecture, theory, nucleotide, e1, test, replacement, trial, e3, sample, amino acid, bayes rule, amino, eq]\t['http://en.wikipedia.org/wiki/Probability_theory', 'http://en.wikipedia.org/wiki/Probability', 'http://en.wikipedia.org/wiki/Bayes%27_theorem', 'http://en.wikipedia.org/wiki/Nucleotide', 'http://en.wikipedia.org/wiki/Multiple_sclerosis', 'http://en.wikipedia.org/wiki/Amino_acid', 'http://en.wikipedia.org/wiki/Dental,_alveolar_and_postalveolar_nasals', 'http://en.wikipedia.org/wiki/Conditional_probability', 'http://en.wikipedia.org/wiki/Set_theory', 'http://en.wikipedia.org/wiki/Omega']\n",
      "6930\tClassification of Web Documents Using a Graph-Based Model \ten\tmp4\t[graph, subgraph, document, contrast, classification, isomorphic, representation, score, common, class, html, node, text, structure, model, datum, mining, different, complexity, edge]\t['http://en.wikipedia.org/wiki/Hello', 'http://en.wikipedia.org/wiki/Lady', 'http://en.wikipedia.org/wiki/Gentleman', 'http://en.wikipedia.org/wiki/Student', 'http://en.wikipedia.org/wiki/University', 'http://en.wikipedia.org/wiki/Electron', 'http://en.wikipedia.org/wiki/Institute_of_technology', 'http://en.wikipedia.org/wiki/Technology', 'http://en.wikipedia.org/wiki/Imaginary_unit', 'http://en.wikipedia.org/wiki/Pleasure']\n",
      "8160\tAdvanced Fluid Dynamics of the Environment\ten\tpdf\t[fluid, mei, layer, temperature, water surface, 2h, homework problem, eddy, diff, insulate, viscosity, induced, 2x, heating, dynamics, amplitude, ex, spatial, differential, cos]\t['http://en.wikipedia.org/wiki/Homework', 'http://en.wikipedia.org/wiki/Problem_solving', 'http://en.wikipedia.org/wiki/Fluid', 'http://en.wikipedia.org/wiki/Fluid_dynamics', 'http://en.wikipedia.org/wiki/Dynamics_(mechanics)', 'http://en.wikipedia.org/wiki/Chiang_Kai-shek', 'http://en.wikipedia.org/wiki/Chiang_C%2e_Mei', 'http://en.wikipedia.org/wiki/Celsius', 'http://en.wikipedia.org/wiki/Prunus_mume', 'http://en.wikipedia.org/wiki/Book_of_Exodus']\n",
      "11812\tLecture 5 - Work-Energy Theorem and Law of Conservation of Energy\ten\tmov\t[force, velocity, minus, energy, function, know, delta, square, integral, time, change, friction, point, let, acceleration, derivative, force act, potential energy, conservation energy, constant]\t['http://en.wikipedia.org/wiki/Conservation_of_energy', 'http://en.wikipedia.org/wiki/Newton_(unit)', 'http://en.wikipedia.org/wiki/Kinetic_energy', 'http://en.wikipedia.org/wiki/Potential_energy', 'http://en.wikipedia.org/wiki/Velocity', 'http://en.wikipedia.org/wiki/Friction', 'http://en.wikipedia.org/wiki/Function_(mathematics)', 'http://en.wikipedia.org/wiki/Gravity', 'http://en.wikipedia.org/wiki/Derivative', 'http://en.wikipedia.org/wiki/Integral']\n",
      "23191\tMedical Decision Support\ten\tpdf\t[clinical, database, db, risk, report, layer, model, workflow, prediction model, users, md, logistic, lesion, renal, architecture, neural networks, death, mi, safety, administrative]\t['http://en.wikipedia.org/wiki/P-value', 'http://en.wikipedia.org/wiki/Swiss_franc', 'http://en.wikipedia.org/wiki/Inline-four_engine', 'http://en.wikipedia.org/wiki/Grade_(climbing)', 'http://en.wikipedia.org/wiki/Kidney', 'http://en.wikipedia.org/wiki/Computer', 'http://en.wikipedia.org/wiki/Database', 'http://en.wikipedia.org/wiki/SQL', 'http://en.wikipedia.org/wiki/Conventional_PCI', 'http://en.wikipedia.org/wiki/Medicine']\n",
      "40540\tDistinguishing Causes from Effects using Nonlinear Acyclic Causal Models\tfr\tmp4\t[causal, nonlinear, model, ica, disturbance, linear, cause, variable, effect, noise, independence, distortion, post, independent, invertible, relation, case, assume, mix, identifiable]\t['http://en.wikipedia.org/wiki/Nonlinear_system', 'http://en.wikipedia.org/wiki/Scatter_plot', 'http://en.wikipedia.org/wiki/Causality', 'http://en.wikipedia.org/wiki/Data_collection', 'http://en.wikipedia.org/wiki/Independent_component_analysis', 'http://en.wikipedia.org/wiki/Causal_model', 'http://en.wikipedia.org/wiki/Variable_(mathematics)', 'http://en.wikipedia.org/wiki/Overfitting', 'http://en.wikipedia.org/wiki/Data_set', 'http://en.wikipedia.org/wiki/Function_(mathematics)']\n"
     ]
    }
   ],
   "source": [
    "! head -10 datasets/catalogue.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6jNmo-qlGMC"
   },
   "source": [
    "### Some requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5wLOS4PJlFha",
    "outputId": "26fe7b8d-327a-4d4b-8086-4d8b2cc5c750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: plotly in /home/mvidaldepalo/.local/lib/python3.8/site-packages (4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/lib/python3.8/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from plotly) (1.14.0)\n",
      "Requirement already up-to-date: ipywidgets in /usr/lib/python3.8/site-packages (7.5.1)\n",
      "Requirement already up-to-date: cufflinks in /home/mvidaldepalo/.local/lib/python3.8/site-packages (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.19.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: plotly>=4.1.1 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: chart-studio>=1.0.0 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlover>=0.2.1 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.4.1 in /usr/lib/python3.8/site-packages (from cufflinks) (44.0.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=5.3.0 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from cufflinks) (7.12.0)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets>=7.0.0 in /usr/lib/python3.8/site-packages (from cufflinks) (7.5.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pandas>=0.19.2->cufflinks) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pandas>=0.19.2->cufflinks) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in /usr/lib/python3.8/site-packages (from plotly>=4.1.1->cufflinks) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3.8/site-packages (from chart-studio>=1.0.0->cufflinks) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (2.5.2)\n",
      "Requirement already satisfied, skipping upgrade: backcall in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (3.0.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (4.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from ipython>=5.3.0->cufflinks) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet>=3.0.2 in /usr/lib/python3.8/site-packages (from requests->chart-studio>=1.0.0->cufflinks) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.5 in /usr/lib/python3.8/site-packages (from requests->chart-studio>=1.0.0->cufflinks) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.21.1 in /usr/lib/python3.8/site-packages (from requests->chart-studio>=1.0.0->cufflinks) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: parso>=0.5.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from jedi>=0.10->ipython>=5.3.0->cufflinks) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from traitlets>=4.2->ipython>=5.3.0->cufflinks) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.3.0->cufflinks) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->cufflinks) (0.1.8)\n",
      "Requirement already up-to-date: pandas in /home/mvidaldepalo/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pandas) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/mvidaldepalo/.local/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install plotly --upgrade\n",
    "! pip install ipywidgets --upgrade\n",
    "! pip install cufflinks --upgrade\n",
    "! pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "yAYIcLQfRYun",
    "outputId": "a6d2d6ed-f940-43ff-ea30-37369c8edb5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Enabling notebook extension plotlywidget/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Known nbextensions:\n",
      "  config dir: /home/mvidaldepalo/.jupyter/nbconfig\n",
      "    notebook section\n",
      "      jupyter-js-widgets/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "      plotlywidget/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "  config dir: /etc/jupyter/nbconfig\n",
      "    notebook section\n",
      "      jupyter-matplotlib/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "      jupyter-js-widgets/extension \u001b[32m enabled \u001b[0m\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbextension enable --py widgetsnbextension\n",
    "! jupyter nbextension enable --py plotlywidget\n",
    "! jupyter nbextension list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXAZLpKVpg--"
   },
   "source": [
    "Some useful imports for the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "SJdfM7oQpgHF",
    "outputId": "73ae667e-5a3e-4647-abbf-e2269689026f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6771b4b778b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m  \u001b[0;31m# often faster than lambda expression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mskdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd  # for easy and effective catalogue manipulation\n",
    "import numpy as np  # for mathematic stuff\n",
    "\n",
    "from ipywidgets import widgets  # for easy implement UI directly in notebook\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from plotly.offline import init_notebook_mode, iplot  # for beautifull plot\n",
    "\n",
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "import cufflinks as cf  # to directly bind pandas and plotly\n",
    "\n",
    "import requests  # for dealing with API\n",
    "import json  # to deal with json inputs/outputs\n",
    "import pprint  # for more friendly console formatting\n",
    "import operator  # often faster than lambda expression\n",
    "\n",
    "import sklearn.metrics.pairwise as skdist\n",
    "import statistics\n",
    "\n",
    "cf.go_offline()  # set plotly to offline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ZS2yjLboLRRP",
    "outputId": "1a1ab4fc-dd62-4f9b-8d73-0ff101b3b0f3"
   },
   "outputs": [],
   "source": [
    "! pip list | grep cufflinks\n",
    "! pip list | grep plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wNknc_U10bgx"
   },
   "source": [
    "### Plotly notebook configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5u-Z_YU4tAy5"
   },
   "source": [
    "Cell configuration\n",
    "\n",
    "This method pre-populates the outputframe with the configuration that Plotly expects and must be executed for every cell which is displaying a Plotly graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZv4zBSfs_en"
   },
   "outputs": [],
   "source": [
    "def enable_plotly_in_cell():\n",
    "    import IPython\n",
    "\n",
    "    display(\n",
    "        IPython.core.display.HTML(\n",
    "            \"\"\"\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "  \"\"\"\n",
    "        )\n",
    "    )\n",
    "    init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBCdww0e-_hN"
   },
   "source": [
    "Plotly Pre-execute Hook\n",
    "\n",
    "If you wish to automatically load the required resources within each cell, you can add the enable_plotly_in_cell function to a Jupyter pre-execute hook and it will be automaticaly executed before any cell execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VMxLCIL-8Rv"
   },
   "outputs": [],
   "source": [
    "# get_ipython().events.register(\"pre_run_cell\", enable_plotly_in_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWVLJTT-_AhD"
   },
   "source": [
    "Because this pre-run hook causes additional javascript resources to be loaded in each cell output, we will disable it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWoHZqJK-8bI"
   },
   "outputs": [],
   "source": [
    "get_ipython().events.unregister('pre_run_cell', enable_plotly_in_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgnlRODTlpcD"
   },
   "source": [
    "## About the catalogue\n",
    "The catalogue is the entry point to the X5GON Content data. You should be able to access it easily. Compute:\n",
    "\n",
    "\n",
    "1. How many different OER in total are there?\n",
    "2. How many videos in French language are there?\n",
    "3. Which are the most popular keywords?\n",
    "4. Which are the most popular concepts?\n",
    "\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "For 1c and 1d you will need to define “most popular” and also decide how many OER you return.\n",
    "\n",
    "For a more complete overview of the pandas library visit the documentation at: https://pandas.pydata.org/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Av7MS76To0SU"
   },
   "outputs": [],
   "source": [
    "list_parser = lambda x: x[1:-1].split(\",\")\n",
    "catalogue = pd.read_csv(\n",
    "    \"datasets/catalogue.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    converters={\"keywords\": list_parser, \"concepts\": list_parser},\n",
    ")\n",
    "# This is added in case initial dataset hasn't the right columns names:\n",
    "catalogue.columns = [\"id\", \"title\", \"language\", \"type\", \"keywords\", \"concepts\"]\n",
    "catalogue.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "colab_type": "code",
    "id": "AMdjYA2K7JXX",
    "outputId": "ae5b0315-fc89-4189-e442-30092ae8ecf6"
   },
   "outputs": [],
   "source": [
    "catalogue.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "4eQw1bj5javN",
    "outputId": "be8fd042-a523-4747-a235-8925d445493d"
   },
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "catalogue[\"language\"].value_counts().iplot(\n",
    "    kind=\"bar\", yTitle=\"Number of resources\", title=\"Type of resources in db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "5mZsJwiA6AF9",
    "outputId": "f146c0ae-7b5a-4f2a-9e42-6ae2e32cb17c"
   },
   "outputs": [],
   "source": [
    "catalogue[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZuh29Z2Zgdc"
   },
   "source": [
    "How many different OER in total are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JVV9rQWlZfXq",
    "outputId": "35130d52-cbe9-4c49-efd0-c6740d5cdfc9"
   },
   "outputs": [],
   "source": [
    "print(f\"There are {catalogue.shape[0]} oers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SbUTN5gZzR9"
   },
   "source": [
    "How many videos in French language are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0VvlQ6sSZzo0",
    "outputId": "63c1aa94-3891-4de4-e664-7ec73fb3a825"
   },
   "outputs": [],
   "source": [
    "videos_extensions = [\"avi\", \"divx\", \"m4v\", \"mpeg\", \"rm\", \"mp4\", \"mov\", \"mpg\"]\n",
    "corresponding_resources = catalogue[\n",
    "    (catalogue[\"language\"] == \"fr\") & (catalogue[\"type\"].isin(videos_extensions))\n",
    "]\n",
    "print(f\"There are {corresponding_resources.shape[0]} which fit to these criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57DXQ127H-Jx"
   },
   "source": [
    "Which are the most popular keywords?\n",
    "\n",
    "The amount of data is rapidly too huge to be treated in notebook and with a personnal computer we will process by random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLSMMoBFILCB"
   },
   "outputs": [],
   "source": [
    "sample_size = 10000  # size of sample set\n",
    "random_state = 42  # for repeteable result\n",
    "keywords_popularity = Counter(\n",
    "    sum(\n",
    "        catalogue[\"keywords\"]\n",
    "        .sample(n=sample_size, random_state=random_state)\n",
    "        .values.flat,\n",
    "        [],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "zw7O-C-oSLO6",
    "outputId": "e5941ddc-5bd9-4c2c-a2ef-a7ccfab87cb6"
   },
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "topNkeywords, topNkcount = zip(*keywords_popularity.most_common(100))\n",
    "fig = go.Figure(\n",
    "    [go.Bar(x=topNkeywords, y=topNkcount)],\n",
    "    layout=go.Layout(\n",
    "        title=\"Most popular keywords\", yaxis=dict(title=\"Keywords occurence\")\n",
    "    ),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LO0uIa6o3T5V"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8I_R08pZWBdu"
   },
   "source": [
    "What about the concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PQ3euRGWNe4"
   },
   "outputs": [],
   "source": [
    "sample_size = 1000  # size of sample set\n",
    "random_state = 42  # for repetable result\n",
    "concepts_popularity = Counter(\n",
    "    sum(\n",
    "        catalogue[\"concepts\"]\n",
    "        .sample(n=sample_size, random_state=random_state)\n",
    "        .values.flat,\n",
    "        [],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "SusKPdAdWNTc",
    "outputId": "e5fb5402-5117-4381-d9f8-ae961a7e1808"
   },
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "topNconcepts, topNccount = zip(*concepts_popularity.most_common(100))\n",
    "fig = go.Figure(\n",
    "    [go.Bar(x=topNconcepts, y=topNccount)],\n",
    "    layout=go.Layout(\n",
    "        title=\"Most popular concepts\", yaxis=dict(title=\"Concept occurence\")\n",
    "    ),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXrWlSEhuOyM"
   },
   "source": [
    "## Using the catalogue and the X5GON API\n",
    "\n",
    "**API Documentation**: https://platform.x5gon.org/products/feed#api\n",
    "\n",
    "The X5GON API allows to get hold of the content and the metadata for a given OER, provided you have its material_id. To find this material_id, you need the catalogue.\n",
    "\n",
    "5. Find the contents of OER # 39642\n",
    "6. Find the metadata of OER # 39642\n",
    "7. Give 10 OERs for concept “Randomness” and for each its License\n",
    "8. Give the 10 most recent OERs which have Random as keyword\n",
    "\n",
    "Hints:\n",
    "\n",
    "For question 6 you can find the production date in the metadata in the JSI API. When you don’t know where to look, try the metadata!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_nBLkTqBP0j"
   },
   "outputs": [],
   "source": [
    "# The X5GON API is available at:\n",
    "PLATFORM_URL = \"https://platform.x5gon.org/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoM2zhd3BLQE"
   },
   "source": [
    "5. Find the contents of OER # 39642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3ykfTiucAVQW",
    "outputId": "2889b3b0-b101-417b-fe88-355ed39b40ce"
   },
   "outputs": [],
   "source": [
    "# initialise the endpoint\n",
    "get_specific_materials_endpoint = \"/oer_materials/{}/contents/\"\n",
    "\n",
    "# get the material id of the first material returned from previous example\n",
    "test_material_id = 39642\n",
    "\n",
    "# query for meta-information about this material, Note that there are no\n",
    "# parameters for this endpoint\n",
    "response = requests.get(\n",
    "    PLATFORM_URL + get_specific_materials_endpoint.format(test_material_id)\n",
    ")\n",
    "\n",
    "# convert the json response to a Python dictionary object for further processing\n",
    "r_json = response.json()\n",
    "pprint.pprint(r_json[\"oer_contents\"][0][\"value\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUHcl3loBvx6"
   },
   "source": [
    "6. Find the metadata of OER # 48331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wR4BMhpABtFu",
    "outputId": "80b89ef9-cab0-4244-ef56-a2bc977d8f7c"
   },
   "outputs": [],
   "source": [
    "# initialise the endpoint\n",
    "get_specific_materials_endpoint = \"/oer_materials/{}\"\n",
    "\n",
    "# get the material id of the first material returned from previous example\n",
    "test_material_id = 48331\n",
    "\n",
    "# query for meta-information about this material, Note that there are no\n",
    "# parameters for this endpoint\n",
    "response = requests.get(\n",
    "    PLATFORM_URL + get_specific_materials_endpoint.format(test_material_id)\n",
    ")\n",
    "\n",
    "# convert the json response to a Python dictionary object for further processing\n",
    "r_json = response.json()\n",
    "print(r_json[\"oer_materials\"][\"metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lro30vdBC1qx"
   },
   "source": [
    "7. Give 10 OERs for concept “Randomness” and for each its License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fUZkOBchC_4F",
    "outputId": "242bde45-38a7-4bcb-e0ab-6baa26fed590"
   },
   "outputs": [],
   "source": [
    "corresponding_resources = catalogue[\n",
    "    [any(\"randomness\" in curl.lower() for curl in v) for v in catalogue[\"concepts\"]]\n",
    "]\n",
    "print(corresponding_resources.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "id": "banHO5JNQKNP",
    "outputId": "b65cfdad-c0eb-4c61-bc99-e436f9d66ac2"
   },
   "outputs": [],
   "source": [
    "corresponding_resources.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFBUfSTiK7VU"
   },
   "outputs": [],
   "source": [
    "corresponding_resources = corresponding_resources.head(10)\n",
    "# initialise the endpoint\n",
    "endpoint = \"/oer_materials/{}\"\n",
    "\n",
    "providers = []\n",
    "licenses = []\n",
    "for material_id in corresponding_resources.index:\n",
    "    response = requests.get(PLATFORM_URL + endpoint.format(material_id))\n",
    "    try:\n",
    "        r_json = response.json()\n",
    "        providers.append(r_json[\"oer_materials\"][\"provider\"][\"provider_name\"])\n",
    "        licenses.append(r_json[\"oer_materials\"][\"license\"])\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Material_id {material_id} error during the request {response}\")\n",
    "        licenses.append(None)\n",
    "        providers.append(None)\n",
    "\n",
    "corresponding_resources[\"license\"] = pd.Series(\n",
    "    licenses, index=corresponding_resources.index\n",
    ")\n",
    "corresponding_resources[\"provider\"] = pd.Series(\n",
    "    providers, index=corresponding_resources.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "id": "LpKy7pEr8nBU",
    "outputId": "40522fa5-d750-45b3-83ab-e1f2f76292b2"
   },
   "outputs": [],
   "source": [
    "corresponding_resources.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv91EougW4bM"
   },
   "source": [
    "Like you can observe the field license is not always present.\n",
    "Indeed, sometimes deal with real data means to deal with problems, be carefull with\n",
    "None value for the future ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfaB4SdyTwLE"
   },
   "source": [
    "8. Give the 10 most recent OERs which have Random as keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nEb-JuQBTxJK",
    "outputId": "4bfa83ad-fe5b-4620-981b-e484fe1abadb"
   },
   "outputs": [],
   "source": [
    "corresponding_resources = catalogue[\n",
    "    [any(\"random\" == curl.lower() for curl in v) for v in catalogue[\"keywords\"]]\n",
    "]\n",
    "# initialise the endpoint\n",
    "endpoint = \"/oer_materials/{}\"\n",
    "print(corresponding_resources.shape)\n",
    "\n",
    "creation_date = []\n",
    "providers = []\n",
    "metadatas = []\n",
    "for material_id in corresponding_resources.index:\n",
    "    response = requests.get(PLATFORM_URL + endpoint.format(material_id))\n",
    "    try:\n",
    "        r_json = response.json()\n",
    "        creation_date.append(r_json[\"oer_materials\"][\"creation_date\"])\n",
    "        providers.append(r_json[\"oer_materials\"][\"provider\"][\"provider_name\"])\n",
    "        metadatas.append(r_json[\"oer_materials\"][\"metadata\"])\n",
    "    except json.JSONDecodeError:\n",
    "        print(\n",
    "            f\"Material_id {material_id} error during the request {response} content:{response.text}\"\n",
    "        )\n",
    "        # Set to None creation_date for failling resources\n",
    "        creation_date.append(None)\n",
    "        providers.append(None)\n",
    "        metadatas.append(None)\n",
    "\n",
    "corresponding_resources[\"creation_date\"] = pd.Series(\n",
    "    creation_date, index=corresponding_resources.index\n",
    ")\n",
    "corresponding_resources[\"provider\"] = pd.Series(\n",
    "    providers, index=corresponding_resources.index\n",
    ")\n",
    "corresponding_resources[\"meta_data\"] = pd.Series(\n",
    "    metadatas, index=corresponding_resources.index\n",
    ")\n",
    "corresponding_resources.sort_values(by=[\"creation_date\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4428C2n0u_1"
   },
   "source": [
    "## Using the catalogue and the LAM API\n",
    "\n",
    "The Nantes API allows to get hold of the models. There are many models. We don’t introduce each of these in the exercises below.\n",
    "\n",
    "9. What are the most important terms for OER # 44900? Does your finding correspond to what is given in the catalogue?\n",
    "10. What are the most important concept for OER # 44900? Does your finding correspond to what is given in the catalogue?\n",
    "11. Compute the distance between OER  # 44900 and OER  # 1234567. What distance have you used? What distances could you use? What representation by vectors have you chosen? Explain your choices?\n",
    "12. Find the OER which concerns concept Machine Learning and is closest to OER #1234567.\n",
    "13. Consider the 100 first OER which have Machine learning as a theme and build an average TF-IDF vector for this class. Then find the OER which is closest to this ideal OER\n",
    "14. Consider the 100 first OER which have Machine learning as a theme and find the class centroid, ie the OER in the class for which $max_{x∈X} d(c,x)$ is minimal. Do you obtain the same OER as in question 3.5?\n",
    "15. Given the set of the 100 first OER which have Machine learning as a theme, which is the simplest? Which is the most complex? Check the answers. Are you convinced\n",
    "\n",
    "Hints.\n",
    "9 Notice that the catalogue gives us \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQlF8S3H2HSk"
   },
   "source": [
    "9. What are the most important terms for OER # 44900? Does your finding correspond to what is given in the catalogue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWh66gmW2N1X"
   },
   "outputs": [],
   "source": [
    "# The X5GON API is available at:\n",
    "PLATFORM_LAM_URL = \"http://wp3.x5gon.org/\"\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "rid = 44900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "b1U4V8gs3IAl",
    "outputId": "23aecde1-f3ae-4f46-8955-772ca7e5597d"
   },
   "outputs": [],
   "source": [
    "endpoint = \"/distance/text2tfidf/fetch\"\n",
    "data = {\"resource_ids\": [rid], \"tfidf_type\": \"SIMPLE\"}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "tfidf = response.json()\n",
    "pprint.pprint(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "qg_bYsE641xQ",
    "outputId": "85db161d-8255-4731-be81-d3c4a4f7996f"
   },
   "outputs": [],
   "source": [
    "top1api = sorted(\n",
    "    tfidf[\"output\"][0][\"value\"][\"value\"].items(), key=operator.itemgetter(1)\n",
    ")[-1]\n",
    "print(\n",
    "    f\"Regarding the API: The more important terms of resource {rid} is '{top1api[0]}' with a tfidf of {top1api[1]}\"\n",
    ")\n",
    "top1catalogue = catalogue[\"keywords\"].loc[rid]\n",
    "print(\n",
    "    f\"Regarding the catalogue: The more important terms of resource {rid} is '{top1catalogue[0]}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiuPJtmq6iP3"
   },
   "source": [
    "It is a match !!! Pratically, the keywords in the catalogue are the tfidf top ranked [1-2]-grams for the given resources. Nevertheless, the catalogue should be considered as an entry point for the api. The latest and more efficient representation would always be avalaible through the API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTDFTQYb7Ze4"
   },
   "source": [
    "10. What are the most important concept for OER # 44900? Does your finding correspond to what is given in the catalogue?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yrFMaPNo7b2W",
    "outputId": "f7ffddb5-f9e0-4e98-8dd1-6247bece1e88"
   },
   "outputs": [],
   "source": [
    "endpoint = \"/distance/wikifier/fetch\"\n",
    "data = {\n",
    "    \"resource_ids\": [rid],\n",
    "    \"wikification_type\": \"SIMPLE\",  # shouldn't be required fix in the API!!!\n",
    "}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "tfidf = response.json()\n",
    "pprint.pprint(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "3Px1wQ-M7lSK",
    "outputId": "50b00e44-3e05-43c0-fa2b-61eac6efa76a"
   },
   "outputs": [],
   "source": [
    "top1api = sorted(\n",
    "    tfidf[\"output\"][0][\"value\"][\"concepts\"], key=operator.itemgetter(\"pageRank\")\n",
    ")[-1]\n",
    "print(\n",
    "    f\"Regarding the API: The more important terms of resource {rid} is '{top1api['url']}' with a pageRank of {top1api['pageRank']}\"\n",
    ")\n",
    "top1catalogue = catalogue[\"concepts\"].loc[rid]\n",
    "print(\n",
    "    f\"Regarding the catalogue: The more important terms of resource {rid} is '{top1catalogue[0]}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1_rF8Fp8hi7"
   },
   "source": [
    "11. Compute the distance between OER  # 44900 and OER  # 3098. What distance have you used? What distances could you use? What representation by vectors have you chosen? Explain your choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "AKnPzvfU8ZJS",
    "outputId": "639e223a-effa-4fa8-9f9d-c15afe09c491"
   },
   "outputs": [],
   "source": [
    "# Here we are computing the distance between the 2 resources basing on their corresponding doc2vec representations (we could use tfidf/wikifier)\n",
    "rids = [44900, 3098]\n",
    "endpoint = \"/distance/doc2vec/fetch/\"\n",
    "data = {\"resource_ids\": rids}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "rids_vectors = response.json()\n",
    "\n",
    "#  from scipy.spatial import distance as scipydist\n",
    "# distance = scipydist.cosine(rids_vectors['output'][0]['value'], rids_vectors['output'][1]['value'])\n",
    "#  print(f\"distance using scipy: {distance}\")\n",
    "distance = skdist.pairwise_distances(\n",
    "    X=[rids_vectors[\"output\"][0][\"value\"], rids_vectors[\"output\"][1][\"value\"]],\n",
    "    metric=\"cosine\",\n",
    "    n_jobs=-1,\n",
    ")[0, 1]\n",
    "print(f\"distance using sklearn pairwise: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DndxXyjljYsZ"
   },
   "source": [
    "**12**. Find the OER which concerns concept Machine Learning and is closest to OER #29302."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "GWHhuzlYjjSx",
    "outputId": "0f250b02-a681-473f-9857-0720e1a68fb1"
   },
   "outputs": [],
   "source": [
    "# Search in the catalogue the resources having the concept \"Machine learning\"\n",
    "principal_oer = 29302\n",
    "corresponding_resources = catalogue[\n",
    "    [\n",
    "        any(\"machine_learning\" in curl.lower() for curl in v)\n",
    "        for v in catalogue[\"concepts\"]\n",
    "    ]\n",
    "]\n",
    "endpoint = \"/distance/doc2vec/fetch/\"\n",
    "data = {\"resource_ids\": [principal_oer]}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "principal_oer_vector = response.json()[\"output\"][0][\"value\"]\n",
    "\n",
    "# Here to avoid putting a big load on the API: we prefere proceeding by batch (knowing that the API already treating by batch)\n",
    "i = 0\n",
    "batch_size = 100\n",
    "rids = []\n",
    "rids_vectors = []\n",
    "rids_vectors_values = []\n",
    "for res in corresponding_resources.index[:100]:\n",
    "    rids.append(res)\n",
    "    i += 1\n",
    "    if i >= batch_size:\n",
    "        data[\"resource_ids\"] = rids\n",
    "        response = requests.post(\n",
    "            PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    "        )\n",
    "        rids_vectors.extend(response.json()[\"output\"])\n",
    "        i = 0\n",
    "        # rids = []\n",
    "if i < batch_size:\n",
    "    data[\"resource_ids\"] = rids\n",
    "    response = requests.post(\n",
    "        PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    "    )\n",
    "\n",
    "    rids_vectors.extend(response.json()[\"output\"])\n",
    "# Compute the distance % oer_principal for all the resources having the concept\n",
    "for rv in rids_vectors:\n",
    "    rv[\"distanceToOer\"] = skdist.pairwise_distances(\n",
    "        X=[principal_oer_vector, rv[\"value\"]], metric=\"cosine\", n_jobs=-1\n",
    "    )[0, 1]\n",
    "# Sort the resources % the computed distance\n",
    "top_closest_resources = sorted(rids_vectors, key=lambda i: i[\"distanceToOer\"])[:10]\n",
    "print(\n",
    "    catalogue.loc[list(map(operator.itemgetter(\"resource_id\"), top_closest_resources))]\n",
    ")\n",
    "# Result\n",
    "endpoint = \"/oer_materials/{}\"\n",
    "oer_pr_metadata = requests.get(PLATFORM_URL + endpoint.format(principal_oer))\n",
    "closest_oer = next(\n",
    "    x[\"resource_id\"] for x in top_closest_resources if x[\"resource_id\"] != principal_oer\n",
    ")\n",
    "oer_cl_metadata = requests.get(PLATFORM_URL + endpoint.format(closest_oer))\n",
    "print(f\"Principal oer:{principal_oer}\")\n",
    "print(f\"Principal oer meta-data:{oer_pr_metadata.json()['oer_materials']}\")\n",
    "print(f\"The closest oer:{closest_oer}\")\n",
    "print(f\"The closest oer meta-data:{oer_cl_metadata.json()['oer_materials']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6O8OmlHoD-m"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TUkI_evjZJQ"
   },
   "source": [
    "13. Consider the 100 first OER which have Machine learning as a theme and build an average TF-IDF vector for this class. Then find the OER which is closest to this ideal OER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcnhJYd0jkLj"
   },
   "outputs": [],
   "source": [
    "# Get the fisrt 100 resources having 'Machine learning'\n",
    "top_closest_resources = sorted(rids_vectors, key=lambda i: i[\"distanceToOer\"])[:100]\n",
    "top_closest_resources_ids = [x[\"resource_id\"] for x in top_closest_resources]\n",
    "# Get their TFIDF vectors\n",
    "endpoint = \"/distance/text2tfidf/fetch\"\n",
    "data = {\"resource_ids\": top_closest_resources_ids, \"tfidf_type\": \"SIMPLE\"}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "closest_tfidf_vectors = response.json()[\"output\"]\n",
    "# Compute the average TF-IDF vector\n",
    "tf_idf_average = dict()\n",
    "for tf in closest_tfidf_vectors:\n",
    "    for ky, val in tf[\"value\"][\"value\"].items():\n",
    "        if ky in tf_idf_average.keys():\n",
    "            tf_idf_average[ky].append(val)\n",
    "        else:\n",
    "            tf_idf_average[ky] = []\n",
    "            tf_idf_average[ky].append(val)\n",
    "tf_idf_average = dict((ky, statistics.mean(val)) for ky, val in tf_idf_average.items())\n",
    "print(tf_idf_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBRro0Zl161x"
   },
   "outputs": [],
   "source": [
    "# Infere the avarage tfidf vector using the lAM API(tfidf knn)\n",
    "endpoint = \"/distance/text2tfidf/knn/vector\"\n",
    "data = {\"vector\": tf_idf_average, \"n_neighbors\": 20}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTlFwp0H16oc"
   },
   "outputs": [],
   "source": [
    "catalogue.loc[response.json()[\"output\"][\"neighbors\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpdW6Cy92Gk5"
   },
   "source": [
    "Some neighbors are not in the catalogue but you can retrieve it using the feed \n",
    "api :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTXOchjAjZfy"
   },
   "source": [
    "14. Consider the 100 first OER which have Machine learning as a theme and find the class centroid, ie the OER in the class for which $min_{x∈X} \\sum d(c,x)$. Do you obtain the same OER as in question 3.5?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wivvUzJjku_"
   },
   "outputs": [],
   "source": [
    "# Get The 100 concerned resources\n",
    "corresponding_resources = catalogue[\n",
    "    [\n",
    "        any(\"machine_learning\" in curl.lower() for curl in v)\n",
    "        for v in catalogue[\"concepts\"]\n",
    "    ]\n",
    "]\n",
    "corresponding_resources = corresponding_resources[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JR5Ck1pldYkb"
   },
   "outputs": [],
   "source": [
    "# Get their tfidf vectors\n",
    "endpoint = \"/distance/text2tfidf/fetch\"\n",
    "data = {\"resource_ids\": corresponding_resources.index.tolist(), \"tfidf_type\": \"SIMPLE\"}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "\n",
    "vectors = [res[\"value\"][\"value\"] for res in response.json()[\"output\"]]\n",
    "ind2concepts = list(set().union(*map(lambda x: set(x.keys()), vectors)))\n",
    "concepts2ind = {v: k for k, v in enumerate(ind2concepts)}\n",
    "\n",
    "\n",
    "def tfidftovect(x):\n",
    "    mat = np.zeros((len(x), len(concepts2ind)))\n",
    "    for i, tfidf in enumerate(x):\n",
    "        for c, v in tfidf.items():\n",
    "            mat[i][concepts2ind[c]] = v\n",
    "    return mat\n",
    "\n",
    "\n",
    "# Computing the representatif matrix for all vectors % all different keywords\n",
    "mat = tfidftovect(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pi6R2gx2eZC9"
   },
   "outputs": [],
   "source": [
    "# Computing the inter-distance between the vectors\n",
    "dist = skdist.pairwise_distances(X=mat, metric=\"cosine\", n_jobs=-1)\n",
    "# Retrieving the centroid resource id\n",
    "centroid_id = corresponding_resources.index.tolist()[dist.sum(axis=0).argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMgXqNDqgsG2"
   },
   "outputs": [],
   "source": [
    "# Retrieving the closest resource to the centroid resource\n",
    "endpoint = \"distance/text2tfidf/knn/res\"\n",
    "data = {\"resource_id\": centroid_id, \"n_neighbors\": 20}\n",
    "\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "catalogue.loc[response.json()[\"output\"][\"neighbors\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xo-VAsZXjZ2Y"
   },
   "source": [
    "15. Given the set of the 100 first OER which have Machine learning as a theme, which is the simplest? Which is the most complex? Check the answers. Are you convinced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTeIs91cjlsk"
   },
   "outputs": [],
   "source": [
    "# Get difficulty scores for the concerned resources\n",
    "endpoint = \"/difficulty/tfidf2technicity/res\"\n",
    "data = {\"resource_ids\": corresponding_resources.index.tolist()}\n",
    "\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ")\n",
    "catalogue.loc[\n",
    "    list(\n",
    "        map(\n",
    "            operator.itemgetter(\"resource_id\"),\n",
    "            sorted(response.json()[\"output\"], key=lambda i: i[\"value\"], reverse=True),\n",
    "        )\n",
    "    )[:10]\n",
    "]\n",
    "# corresponding_resources.sort_values(\"/difficulty/tfidf2technicity/res\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxQt51-I04Tr"
   },
   "source": [
    "## Using both and going further\n",
    "\n",
    "16. Find a long resource, and for this resource extract its continuous wikifier vector. Then choose the 5 most important concepts and plot their evolution through time.\n",
    "17. Let us suppose that a learning goal is described by 2 concepts and 3 keywords. Suggest a playlist of OER to be watched\n",
    "18. Given a set of resources and a time constraint (2 hours), suggest a playlist consistent with the constraint.\n",
    "19. Given a set of OER, which is the odd one out?\n",
    "\n",
    "Hints:\n",
    "\n",
    "17 This is a more complex operation. To find a long OER you will need to use the catalogue to build an iterator over Ids, then count the number of words in each transcription. Once this is done, the continuous wikifier vector can be obtained using the LAM API. You now have to decide what are the 5 most important concepts: one possible answer can be obtained through averaging the scores. Then plotting is a good idea to better visualize the results. \n",
    "\n",
    "18 Now things are becoming more difficult. Don’t go for the best solution. Just for one which returns a result. We know how to build a vector (TF-IDF, Word2Vec, Wikifier) for each OER. But can we build one from 2 concepts and 3 keywords?\n",
    "\n",
    "19 Even harder! We first have to associate a consumption length to each OER. How do we do that? For a video, we could take the duration, but that would not take care of the fact that we may want to check some parts of the video various times. For a pdf, we have to decide how long it takes to read 100 words. Perhaps all users are different? Perhaps we can learn their consumption speed? And we can also see which OERs should come first (or last) through a careful use of the Next-OER service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Hivn8cNkDnQ"
   },
   "source": [
    "16. Find a long resource, and for this resource extract its continuous wikifier vector. Then choose the 5 most important concepts and plot their evolution through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJxpZ4E0YHBp"
   },
   "outputs": [],
   "source": [
    "enable_plotly_in_cell()\n",
    "endpoint = \"/temporal/continuouswikifier/fetch\"\n",
    "data = {\"resource_ids\": [29302], \"wikification_type\": \"SIMPLE\"}\n",
    "response = requests.post(\n",
    "    PLATFORM_LAM_URL + endpoint, headers=HEADERS, data=json.dumps(data)\n",
    ").json()\n",
    "\n",
    "keyconcepts = [\n",
    "    Counter({cdict[\"title\"]: cdict[\"pageRank\"] for cdict in chunk[\"concepts\"]})\n",
    "    for chunk in response[\"output\"][0][\"value\"]\n",
    "]\n",
    "\n",
    "top20, _ = zip(*sum(keyconcepts, Counter()).most_common(10))\n",
    "x = [f\"chunks{i}\" for i in range(len(keyconcepts))]\n",
    "y = {t: [count[t] for count in keyconcepts] for t in top20}\n",
    "data = [go.Bar(x=x, y=y[cname], name=cname) for cname in top20]\n",
    "layout = go.Layout(\n",
    "    barmode=\"group\",\n",
    "    title=catalogue.loc[29302][\"title\"],\n",
    "    xaxis=dict(title=\"Concepts\", automargin=True),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.update_layout(barmode=\"stack\")\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPZ9c89AjBeR"
   },
   "source": [
    "## Now it's time to try by yourself !!!\n",
    "\n",
    "On your own ideas, or in the use cases below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qmv4UvfkEIj"
   },
   "source": [
    "17. Let us suppose that a learning goal is described by 2 concepts and 3 keywords. Suggest a playlist of OER to be watched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-pGRrsCYJDa"
   },
   "outputs": [],
   "source": [
    "learning_goal_concepts = [\n",
    "    \"https://en.wikipedia.org/wiki/Open_educational_resources\",\n",
    "    \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "]\n",
    "learning_goal_keywords = [\"Education\", \"Open\", \"Network\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m32_7LRJkEpz"
   },
   "source": [
    "18. Given a set of resources and a time constraint (2 hours), suggest a playlist consistent with the constraint.\n",
    "\n",
    "hint:\n",
    "https://en.wikipedia.org/wiki/Words_per_minute\n",
    "\n",
    "Audiobooks are recommended to be 150–160 words per minute, which is the range that people comfortably hear and vocalize words.\n",
    "Slide presentations tend to be closer to 100–125 wpm for a comfortable pace, auctioneers can speak at about 250 wpm, and the fastest speaking policy debaters speak from 350 to over 500 words per minute. Internet speech calculators show that various things influence words per minute including nervousness.\n",
    "\n",
    "John Moschitta, Jr., was listed in Guinness World Records, for a time, as the world's fastest speaker, being able to talk at 586 wpm. He has since been surpassed by Steve Woodmore, who achieved a rate of 637 wpm. \n",
    "\n",
    "The definition of each \"word\" is often standardized to be five characters or keystrokes long in English, including spaces and punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3w3NMBkDYKs1"
   },
   "outputs": [],
   "source": [
    "setids = {\n",
    "    111935,\n",
    "    112048,\n",
    "    113263,\n",
    "    112748,\n",
    "    111645,\n",
    "    52912,\n",
    "    15519,\n",
    "    109653,\n",
    "    112861,\n",
    "    105938,\n",
    "    43770,\n",
    "    65548,\n",
    "    114821,\n",
    "    111773,\n",
    "    114977,\n",
    "    115356,\n",
    "    113124,\n",
    "    114785,\n",
    "    105997,\n",
    "    111072,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVo7op_RfSgp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2d-0eKEPkFAG"
   },
   "source": [
    "\n",
    "19. Given a set of OER, which is the odd one out? and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCQTkC4RYLz6"
   },
   "outputs": [],
   "source": [
    "setids = {\n",
    "    65490,\n",
    "    16602,\n",
    "    16737,\n",
    "    58512,\n",
    "    72491,\n",
    "    48968,\n",
    "    64639,\n",
    "    48716,\n",
    "    16734,\n",
    "    17501,\n",
    "    65546,\n",
    "    47129,\n",
    "    17413,\n",
    "    58280,\n",
    "    49717,\n",
    "    44982,\n",
    "    43405,\n",
    "    49178,\n",
    "    49301,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy8n9I10fR2S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRLatrXEjXd3"
   },
   "source": [
    "## Build an interactive notebook using ipywidgets\n",
    "\n",
    "\n",
    "Query API and dissect the json output \n",
    "is of course a real pleasure for all computer scientists. To do the job, console may be sufficient.\n",
    "\n",
    "But everybody like the more friendly output. And in the hackathon context the challenge is to present what you've done to external people. A friendly output becomes so far essential. \n",
    "\n",
    "An efficient way to transform our ugly notebook in a beautifull interactive one is to use the ipywidgets library.\n",
    "\n",
    "The following questions will give you some baselines for handling this technology.\n",
    "\n",
    "**ipywidgets documention**: https://ipywidgets.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNHfZUzJm8Gw"
   },
   "source": [
    "20. Build your first dropdown ! The goal is to display a dropdown of resource titles and to display each time a resource is selected its corresponding concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCmxpz7dm5r0"
   },
   "outputs": [],
   "source": [
    "# Create the dropdown\n",
    "tDropdown = widgets.Dropdown(\n",
    "    options=catalogue[\"title\"].head(100), index=0, description=\"Resources:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hE93EYxBsFf8"
   },
   "outputs": [],
   "source": [
    "# Create an output text area\n",
    "outConcepts = widgets.Output()  # Text output for concepts plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTz9wetWqcqA"
   },
   "outputs": [],
   "source": [
    "# Allow to clear the text area and to handle all the function print\n",
    "@outConcepts.capture(clear_output=True)\n",
    "def on_tDropdown_selection(index):\n",
    "    pprint.pprint(catalogue.iloc[index[\"new\"]].concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBDE_1TFqTJs"
   },
   "outputs": [],
   "source": [
    "# Bind the function with the selection events\n",
    "tDropdown.observe(on_tDropdown_selection, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67QIggjGrlrg"
   },
   "outputs": [],
   "source": [
    "# Finally display the whole\n",
    "display(tDropdown, outConcepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9zaMxCvtqCm"
   },
   "source": [
    "21. Update a text area is funny but not really satisfaying. Could we do the same thing with a chart ? Plot the bar of concepts importance for the selected resource.\n",
    "**This section still in devellopment, mainly due to compatibility problems between google collaboratory and FigureWidget, but still completely working if you execute the code on your local environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N65JIAOVGf9M"
   },
   "outputs": [],
   "source": [
    "# Create the dropdown\n",
    "enable_plotly_in_cell()\n",
    "tDropdown2 = widgets.Dropdown(\n",
    "    options=catalogue[\"title\"].head(100), index=0, description=\"Resources:\"\n",
    ")\n",
    "concepts = catalogue.iloc[0].concepts\n",
    "print(type(concepts), concepts)\n",
    "vals = np.random.rand(len(concepts))\n",
    "print(type(vals), vals.tolist())\n",
    "\n",
    "\n",
    "radar = go.FigureWidget(\n",
    "    data=[go.Scatterpolar(theta=concepts, r=vals.tolist(), fill=\"toself\")],\n",
    "    layout=go.Layout(\n",
    "        title=\"Most popular concepts\", yaxis=dict(title=\"Concept occurence\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDgge5_BvUbD"
   },
   "outputs": [],
   "source": [
    "# Allow to clear the text area and to handle all the function print\n",
    "def on_tDropdown_selection2(index):\n",
    "    concepts = catalogue.iloc[index[\"new\"]].concepts\n",
    "    radar.data[0].r = np.random.rand(len(concepts)).tolist()\n",
    "    radar.data[0].theta = concepts\n",
    "    radar.data[0].name = tDropdown.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrKv0Ycdv3tv"
   },
   "outputs": [],
   "source": [
    "# Bind the function with the selection events\n",
    "tDropdown2.observe(on_tDropdown_selection2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bPHjJU4xzKp"
   },
   "outputs": [],
   "source": [
    "# Finally display the whole\n",
    "display(tDropdown2, radar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsJoogOB1rnN"
   },
   "source": [
    "## Need of technical support\n",
    "\n",
    "Use the Slack channel for all your queries!!\n",
    "\n",
    "Also, feel free to contact:\n",
    "- walid.benromdhane@univ-nantes.fr\n",
    "- victor.connes@univ-nantes.fr\n",
    "- m.bulathwela@ucl.ac.uk"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "X5GON Hackathon Nantes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "x5gon_hackathon",
   "language": "python",
   "name": "x5gon_hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
